# -*- coding: utf-8 -*-
"""StumbleuponChallenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14dGfbbhlZVtu3krv6ARhHjnZrgsnjbdd

###Assignment
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np 

import matplotlib.pyplot as plt 
import seaborn as sns 
# %matplotlib inline 

import tensorflow as tf
import re

"""##Data Visualization"""

df_train=pd.read_csv('/content/drive/MyDrive/StumbleUpon/train.tsv',sep='\t')

df_test_main=pd.read_csv('/content/drive/MyDrive/StumbleUpon/test.tsv',sep='\t',usecols=['urlid','boilerplate'])

df_train.columns

df_train.head()

df_train['alchemy_category'].value_counts()

plt.figure(figsize=(15,10))
sns.countplot(x=df_train['alchemy_category'],hue=df_train['label']);
plt.xlabel('Category');
plt.xticks(rotation=90);

sns.countplot(x=df_train['label'])

"""##Data Pre-processing and cleaning"""

def clean_text(
    string: str, 
    punctuations=r'''!()-[]{};:'"\,<>./?@#$%^&*_~''',
    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:
    """
    A method to clean text 
    """
    # Cleaning the urls
    string = re.sub(r'https?://\S+|www\.\S+', '', string)

    # Cleaning the html elements
    string = re.sub(r'<.*?>', '', string)

    # Removing the punctuations
    for x in string.lower(): 
        if x in punctuations: 
            string = string.replace(x, "") 

    # Converting the text to lower
    string = string.lower()

    # Removing stop words
    string = ' '.join([word for word in string.split() if word not in stop_words])

    # Cleaning the whitespaces
    string = re.sub(r'\s+', ' ', string).strip()

    return string

df_train['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_train['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)

df_test_main['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_test_main['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)

for i in range(0,len(df_train['boilerplate'])):
  df_train['boilerplate'][i] = clean_text(df_train['boilerplate'][i])

for i in range(0,len(df_test_main['boilerplate'])):
  df_test_main['boilerplate'][i] = clean_text(df_test_main['boilerplate'][i])

"""##Train Test Split"""

import numpy as np
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(df_train['boilerplate'], df_train['label'], test_size=0.3, random_state=4
)

"""##Model Training"""

from gensim.models import Word2Vec
import time
import numpy as np
# Skip-gram model (sg = 1)
size = 1000
window = 3
min_count = 1
workers = 3
sg = 1

word2vec_model_file = '/content/drive/MyDrive/StumbleUpon/Model' + 'word2vec_' + str(size) + '.model'
start_time = time.time()
stemmed_tokens = pd.Series(df_train['boilerplate']).values
# Train the Word2Vec Model
w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)
print("Time taken to train word2vec model: " + str(time.time() - start_time))
w2v_model.save(word2vec_model_file)

# Load the model from the model file
sg_w2v_model = Word2Vec.load('/content/drive/MyDrive/StumbleUpon/Model/Modelword2vec_1000.model')

"""##Saving word embeddings"""

word2vec_filename = '/content/drive/MyDrive/StumbleUpon/Embeddings' + 'train_review_word2vec.csv'
with open(word2vec_filename, 'w+') as word2vec_file:
    for index, row in X_train.to_frame().iterrows():
        model_vector = (np.mean([sg_w2v_model[token] for token in row['boilerplate']], axis=0)).tolist()
        if index == 0:
            header = ",".join(str(ele) for ele in range(1000))
            word2vec_file.write(header)
            word2vec_file.write("\n")
        # Check if the line exists else it is vector of zeros
        if type(model_vector) is list:  
            line1 = ",".join( [str(vector_element) for vector_element in model_vector] )
        else:
            line1 = ",".join([str(0) for i in range(1000)])
        word2vec_file.write(line1)
        word2vec_file.write('\n')

"""## Fitting the Model"""

import time
#Import the DecisionTreeeClassifier
from sklearn.tree import DecisionTreeClassifier
# Load from the filename
word2vec_df = pd.read_csv('/content/drive/MyDrive/StumbleUpon/Embeddings/Embeddingstrain_review_word2vec.csv')
#Initialize the model
clf_decision_word2vec = DecisionTreeClassifier()

start_time = time.time()
# Fit the model
clf_decision_word2vec.fit(word2vec_df, Y_train)
print("Time taken to fit the model with word2vec vectors: " + str(time.time() - start_time))

"""##Calculating Accuracy"""

from sklearn.metrics import classification_report
test_features_word2vec = []
for index, row in X_test.to_frame().iterrows():
    model_vector = np.mean([sg_w2v_model[token] for token in row['boilerplate']], axis=0)
    if type(model_vector) is list:
        test_features_word2vec.append(model_vector)
    else:
        test_features_word2vec.append(np.array([0 for i in range(1000)]))
test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)
print(classification_report(Y_test,test_predictions_word2vec))

"""##Final Prediction"""

test_features_word2vec = []
for index, row in df_test_main.iterrows():
    model_vector = np.mean([sg_w2v_model[token] for token in row['boilerplate']], axis=0)
    if type(model_vector) is list:
        test_features_word2vec.append(model_vector)
    else:
        test_features_word2vec.append(np.array([0 for i in range(1000)]))
test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)